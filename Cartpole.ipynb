{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for the paper titled \"Shallow Planning under Partial Observability\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install gymnasium\n",
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install pymdptoolbox\n",
    "%pip install joblib\n",
    "%pip install stable-baselines3\n",
    "\n",
    "# Import necessary libraries\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "import gc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_schedule(initial_value: Union[float, str]) -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "\n",
    "    :param initial_value: (float or str)\n",
    "    :return: (function)\n",
    "    \"\"\"\n",
    "    # Force conversion to float\n",
    "    initial_value_ = float(initial_value)\n",
    "\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0\n",
    "        :param progress_remaining: (float)\n",
    "        :return: (float)\n",
    "        \"\"\"\n",
    "        return progress_remaining * initial_value_\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper for the CartPole environment to add noise\n",
    "class NoisyCartPoleEnv(gym.Wrapper):\n",
    "    def __init__(self, env, noise_std=0.1):\n",
    "        super(NoisyCartPoleEnv, self).__init__(env)\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        state, info = self.env.reset(**kwargs)\n",
    "        noisy_state = self.add_noise(state)\n",
    "        return noisy_state, info\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "        noisy_next_state = self.add_noise(next_state)\n",
    "        return noisy_next_state, reward, terminated, truncated, info\n",
    "\n",
    "    def add_noise(self, state):\n",
    "        if self.noise_std > 0:\n",
    "            noise = np.random.normal(0, self.noise_std, size=state.shape)\n",
    "            return state + noise\n",
    "        return state\n",
    "\n",
    "def make_noisy_env(env_id, rank, seed=0, noise_std=0.1):\n",
    "    def _init():\n",
    "        env = gym.make(env_id)\n",
    "        env = NoisyCartPoleEnv(env, noise_std)\n",
    "        env.reset(seed=seed + rank)\n",
    "        return env\n",
    "    return _init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_environment(model, seed,std, max_steps=1000):\n",
    "    '''\n",
    "    q_values : (n, n, k) numpy array where n is the number of (discrete) states and k is the number of actions\n",
    "    seed : int, seed for the random number generator\n",
    "    '''\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    env = NoisyCartPoleEnv(env, std)\n",
    "\n",
    "    time_step = 0\n",
    "    terminated, truncated = False, False\n",
    "    obs, _ = env.reset(seed=seed)\n",
    "    total_reward = 0  # Initialize total reward\n",
    "    \n",
    "    while not (terminated or truncated) and time_step < max_steps:\n",
    "        env.render()\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)            \n",
    "        total_reward += reward  # Accumulate the reward\n",
    "        time_step += 1\n",
    "    \n",
    "    env.close()\n",
    "    return total_reward  # Return the total reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main loop for training the models\n",
    "stds = [0.0,0.1,1]\n",
    "gammas = np.linspace(0, 0.98, 5)\n",
    "\n",
    "learning_rate_schedule = linear_schedule(1e-3)\n",
    "clip_range_schedule = linear_schedule(0.2)\n",
    "n_simuls = 10\n",
    "for simul in range(n_simuls):\n",
    "    for i, std in enumerate(stds):\n",
    "        for j, gamma in enumerate(gammas):\n",
    "            print(\"doing std\" + str(std) + \"with gamma:\" + str(gamma)+ \"simluation: \" + str(simul))\n",
    "\n",
    "            # Number of parallel environments\n",
    "            n_envs = 8\n",
    "            env_id = \"CartPole-v1\"\n",
    "\n",
    "            # Create the vectorized environment\n",
    "            env = SubprocVecEnv([make_noisy_env(env_id, i, noise_std=std) for i in range(n_envs)])\n",
    "\n",
    "            # Initialize the PPO model with the vectorized environment\n",
    "            model = PPO(\n",
    "                policy='MlpPolicy',\n",
    "                env=env,\n",
    "                n_steps=32,\n",
    "                batch_size=256,\n",
    "                gamma=gamma,\n",
    "                gae_lambda=0.8,\n",
    "                n_epochs=20,\n",
    "                ent_coef=0.0,\n",
    "                learning_rate=linear_schedule(1e-3),\n",
    "                clip_range=linear_schedule(0.2),\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            # Learn with the specified number of timesteps\n",
    "            model.learn(total_timesteps=100000, log_interval=10)\n",
    "            model.save(f\"models/simul{simul}/CartpoleGamma{round(gamma, 2)}std{std}\")\n",
    "            del model\n",
    "            env.close()\n",
    "            del env\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the models into memory\n",
    "stds = [0.0,0.1,1]\n",
    "gammas = np.linspace(0, 0.98, 5)  # Example values for gamma\n",
    "n_simuls = 10\n",
    "import numpy as np\n",
    "loaded_models = np.empty((n_simuls, len(stds), len(gammas)), dtype=object)\n",
    "\n",
    "for simul in range(n_simuls):\n",
    "    for i, std in enumerate(stds):\n",
    "        for j, gamma in enumerate(gammas):\n",
    "            model_path = f\"models/simul{simul}/CartpoleGamma{round(gamma, 2)}std{std}\"\n",
    "            loaded_models[simul, i, j] = PPO.load(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the average reward for each model depending on the noise and gamma\n",
    "seeds = range(100)\n",
    "rewards = np.zeros((n_simuls, len(stds), len(gammas), len(seeds)))\n",
    "\n",
    "def get_reward(model, std, seed):\n",
    "    return play_environment(model, seed, std, 500)\n",
    "\n",
    "for seed in seeds:\n",
    "    for simul in range(n_simuls):\n",
    "        for i, std in enumerate(stds):\n",
    "            for j, gamma in enumerate(gammas):\n",
    "                print(\"doing simul: \" + str(simul) + \"with std: \" + str(std) + \"and gamma: \" + str(gamma) + \"seed: \" + str(seed))\n",
    "                rewards[simul, i, j, seed] = get_reward(loaded_models[simul, i, j], std, seed)\n",
    "\n",
    "# Reshape the results back into the original 4D array shape\n",
    "rewards = np.array(rewards).reshape(n_simuls, len(stds), len(gammas), len(seeds))\n",
    "\n",
    "# Calculate mean across seeds, then across POMDPs and simulations\n",
    "mean_rewards = rewards.mean(axis=3)  # Mean across seeds\n",
    "mean_rewards = mean_rewards.mean(axis=0)  # Mean across simulations\n",
    "\n",
    "# Calculate mean and std deviation across seeds, then across POMDPs and simulations\n",
    "std_rewards = rewards.std(axis=3)  # Std deviation across seeds\n",
    "std_rewards = std_rewards.mean(axis=0)  # Mean across simulations\n",
    "\n",
    "fig1, ax1 = plt.subplots(figsize=(5.5, 4))\n",
    "x = np.arange(len(gammas))\n",
    "markers = ['o', 's', 'D', 'x']  # Circle, square, diamond, star, x\n",
    "\n",
    "for i, std in enumerate(stds):\n",
    "    ax1.errorbar(x, mean_rewards[i], yerr=std_rewards[i], label=f'std={std}', marker=markers[i % len(markers)], markersize=8, capsize=5)\n",
    "\n",
    "ax1.set_xlabel('$\\gamma$', fontsize=10)\n",
    "ax1.set_ylabel('Average Reward', fontsize=10)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([f'{g:.2f}' for g in gammas], fontsize=10)\n",
    "ax1.legend(fontsize=10)\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "plt.savefig('average_reward_plot.pdf')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print the average rewards for gamma = 1 (last column) for reference\n",
    "print(\"Average rewards for gamma = 1 (across all simulations):\")\n",
    "for i, std in enumerate(stds):\n",
    "    print(f\"std={std}: {mean_rewards[i, -1]:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
